{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b9cba8b-37d9-4dcf-ac29-3afc47098eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections.abc import Callable, Iterable\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from random import random, shuffle\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f242dba6-0525-4425-baec-3b41310d44f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_csv(path: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Loads every csv file in the path.\"\"\"\n",
    "    dataloader = {file_path.split(\"/\")[-1].split(\".\")[0]: pd.read_csv(file_path, header=None, sep=\" \") for file_path in glob(path+\"*.csv\")}\n",
    "    return dataloader\n",
    "\n",
    "@dataclass\n",
    "class Model(ABC):\n",
    "    \"\"\"Base Representation of a Machine Learning Model.\"\"\"\n",
    "    loss: Optional[Callable[[np.ndarray, np.ndarray], np.ndarray]]\n",
    "    optimizer: Optional[Callable[[np.ndarray, np.ndarray, np.ndarray,], (np.ndarray, float)]]\n",
    "    hist: Dict[str, List]\n",
    "    learning_rate: float \n",
    "    epochs: int\n",
    "    eval_step: int\n",
    "    early_stop: bool\n",
    "    range_flag: bool\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def predict(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Predicts the out come of Matrix X.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractclassmethod\n",
    "    def update_weights(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Updates the weights of the model after one step or epoch.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray ) -> None:\n",
    "        \"\"\"trains the model using the vector features X and labels Y.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def save_hist(self, X: np.ndarray, y: np.ndarray, i: int) -> None:\n",
    "        \"\"\"Saves the parameters of the model in between updates of training.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def plot_training(self) -> go.Figure:\n",
    "        \"\"\"Animate the change of prediction of the model on training data with dimension of 1.\"\"\"\n",
    "        assert self.range_flag == False\n",
    "        fig=go.Figure().set_subplots(1,1, vertical_spacing=0.05,\n",
    "                             specs=[[{\"type\": \"scatter\", \"secondary_y\": False}]])\n",
    "        \n",
    "        # first frame of the plot\n",
    "        fig.add_trace(go.Scatter(x=self.hist[\"x_range\"], y=self.hist[\"y_range\"], name=\"train\", mode='markers'), 1,1)\n",
    "        fig.add_trace(go.Scatter(x=self.hist[\"x_range\"], y=self.hist[\"y_range\"], name=\"train\", mode='markers'), 1,1);\n",
    "\n",
    "        fig.update(frames=self.hist[\"frame\"])\n",
    "        \n",
    "        # details about the plot layout\n",
    "        sliders_dict = {\"active\": 0, \"yanchor\": \"top\", \"xanchor\": \"left\",\n",
    "                        \"currentvalue\": { \"font\": {\"size\": 20}, \"prefix\": \"Year:\", \"visible\": True, \"xanchor\": \"right\"},\n",
    "                        \"transition\": {\"duration\": 300, \"easing\": \"cubic-in-out\"}, \"pad\": {\"b\": 10, \"t\": 50}, \"len\": 0.9, \n",
    "                        \"x\": 0.1, \"y\": 0, \"steps\": []}\n",
    "\n",
    "        updatemenus_dict = {\n",
    "                \"buttons\": [{\"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": False}, \"fromcurrent\": True,\n",
    "                \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}}], \"label\": \"Play\", \"method\": \"animate\"},\n",
    "                {\"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\", \"transition\": {\"duration\": 0}}],\n",
    "                \"label\": \"Pause\", \"method\": \"animate\"}], \"direction\": \"left\", \"pad\": {\"r\": 10, \"t\": 87}, \"showactive\": True,\n",
    "                \"type\": \"buttons\", \"x\": 0.1, \"xanchor\": \"right\", \"y\": 0, \"yanchor\": \"top\",\n",
    "                }\n",
    "\n",
    "        fig.update_layout(sliders=[sliders_dict],\n",
    "                          updatemenus = [updatemenus_dict])\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def plot_history(self, keys: Iterable) -> go.Figure:\n",
    "        \"\"\"Plots the keys saved in self.hist for each epoch.\"\"\"\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        for key in keys:\n",
    "            fig.add_trace(go.Scatter(x=np.arange(stop=self.epochs,),\n",
    "                                     y=self.hist[key], name=key))\n",
    "        return fig\n",
    "\n",
    "    \n",
    "class Scaler():\n",
    "    mean: float\n",
    "    std: float\n",
    "    def set_standard_scaler(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Standardize features by removing the mean and scaling to unit variance and save it for prediction time.\"\"\"\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        standard = (X - self.mean) / self.std\n",
    "\n",
    "        return standard\n",
    "\n",
    "    def get_standard_scaler(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Standardize features by removing the mean of training set and scaling to unit variance of training set.\"\"\"\n",
    "        standard = (X - self.mean) / self.std\n",
    "\n",
    "        return standard\n",
    "\n",
    "@dataclass\n",
    "class Layer(ABC):\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"computes feed forward of Layer respect to input X\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def backward_propagation(self, pred, learning_rate):\n",
    "        \"\"\"computes backward propagation of Layer respect to input X\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class ActivationFunction(Layer):\n",
    "    activation: Callable[[np.ndarray], np.ndarray]\n",
    "    activation_prime: Callable[[np.ndarray], np.ndarray]\n",
    "    \n",
    "    def random_initializers(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def feed_forward(self, x) -> np.ndarray:\n",
    "        self.x = x.copy()\n",
    "        return self.activation(self.x)\n",
    "\n",
    "    def backward_propagation(self, dy: np.ndarray) -> np.ndarray:\n",
    "        dx = self.activation_prime(self.x) * dy\n",
    "        return dx\n",
    "\n",
    "@dataclass\n",
    "class FCLayer(Layer):\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    activation: Callable[[np.ndarray], np.ndarray]\n",
    "    activation_prime: Callable[[np.ndarray], np.ndarray]\n",
    "    \n",
    "    def random_initializers(self, std=5e-2) -> None:\n",
    "        \"\"\"Make a random start for weights and bias\"\"\"\n",
    "        self.weights = std * np.random.randn(self.input_size, self.output_size)\n",
    "        self.bias =  np.zeros((1, self.output_size))\n",
    "        \n",
    "    def feed_forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"computes feed forward of Layer respect to input X\"\"\"\n",
    "        # print(f\"W.T={self.weights.T.shape}, x.T={x.T.shape}, b= {self.bias.shape}\")\n",
    "        self.x = x.copy()\n",
    "        self.z =  np.dot(x,self.weights)  + self.bias\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a\n",
    "        \n",
    "    def backward_propagation(self, dy: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "        \"\"\"computes backward propagation of Layer respect to input X\"\"\"\n",
    "        # # print(f\"output.shape= {next_layer_error.shape}\")\n",
    "        # seperate activation function from layer because there were a logical bug I couldn't fix\n",
    "        db = dy * self.activation_prime(self.a)\n",
    "        # print(f\"error= {error.shape}, W.T= {self.weights.T.shape}\")\n",
    "        dx =  np.dot(dy, self.weights.T)\n",
    "        # print(f\"x.T={self.x.T.shape}, error= {error.shape}\")\n",
    "        dW = np.dot(self.x.T, dy)\n",
    "        \n",
    "\n",
    "        return dW, db, dx\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class MLP(Model):\n",
    "    \"\"\"Simple MLP Impelementation\"\"\"\n",
    "    layers: List[Layer]\n",
    "    loss_prime: Callable[[np.ndarray], np.ndarray]\n",
    "    \n",
    "    def zero_grads(self) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.x= 0\n",
    "            layer.z= 0\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer.feed_forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dy: float) -> (List[float], List[float]):\n",
    "        output = dy\n",
    "        dW, db = [], []\n",
    "        for layer in self.layers[::-1]:\n",
    "            output = layer.backward_propagation(output)\n",
    "            dW.append(output[0])\n",
    "            db.append(output[1])\n",
    "            output = output[2]\n",
    "            \n",
    "        return dW, db\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> List[float]:\n",
    "        \"\"\"Predicts the out come of Matrix X.\"\"\"\n",
    "        preds = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            preds.append(self.forward(X[i]))\n",
    "\n",
    "        return preds\n",
    "        \n",
    "    def update_weights(self, dW: List[float], db: List[float]) -> None:\n",
    "        \"\"\"Updates the weights of the model after one step or epoch.\"\"\"\n",
    "\n",
    "        # print(f\"#layers= {len(self.layers)}, #weights_error:{len(weights_error)}, #error:{len(error)}\")\n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            # print(f\"dW[{i}] = {dW[i]}, db[{i}] = {db[i]}\")\n",
    "            layer.weights = layer.weights - (self.learning_rate * dW[i])\n",
    "            layer.bias = layer.bias - (self.learning_rate * db[i])\n",
    "            \n",
    "    def random_initializers(self) -> None:\n",
    "        \"\"\"Make a random start for weights and bias\"\"\"\n",
    "        [layer.random_initializers() for layer in self.layers]\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray ) -> None:\n",
    "        \"\"\"trains the model using the vector features X and labels Y.\"\"\"\n",
    "        self.hist={\"weights\":[],\"bias\":[],\"loss\":[0], \"frame\":[]}\n",
    "        # random start\n",
    "        self.random_initializers()\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            loss= 0\n",
    "            # for each sample in training set feed-forward and backpropagate\n",
    "            for i, x in enumerate(X):\n",
    "                self.zero_grads()\n",
    "                # feed-forward\n",
    "                # x = X[i].reshape((X[i].shape[0], 1))\n",
    "                # print(f\"X[i].shape:{x.shape}\")\n",
    "                pred = self.forward(x)\n",
    "                # add loss \n",
    "                loss+= self.loss(y[i], pred)\n",
    "                # compute gradient update\n",
    "                dy = self.loss_prime(y[i], pred)\n",
    "                # backward propagation\n",
    "                dW, db = self.backward(dy)\n",
    "                # update weights\n",
    "                self.update_weights(dW= dW, db= db)\n",
    "            \n",
    "            # save epoch train history\n",
    "            self.save_hist(X= X, y= y, i= epoch, loss= loss)\n",
    "            if self.hist[\"loss\"][-1] > self.hist[\"loss\"][-2] and self.early_stop:\n",
    "                break\n",
    "\n",
    "    def save_hist(self, X: np.ndarray, y: np.ndarray, i: int, loss: float,) -> None:\n",
    "        \"\"\"Saves the parameters of the model in between updates of training.\"\"\"\n",
    "        self.hist[\"loss\"].append(loss/X.shape[0])\n",
    "        self.hist[\"bias\"].append([layer.bias for layer in self.layers])\n",
    "        self.hist[\"weights\"].append([layer.weights for layer in self.layers])\n",
    "        print(f'Training Loss for epoch {i}: {self.hist[\"loss\"][-1]}')\n",
    "        \n",
    "        if X.shape[1] == 1:\n",
    "            if self.range_flag:\n",
    "                self.hist[\"x_range\"] = [min(X), max(X)]\n",
    "                self.hist[\"y_range\"] = [min(y), max(y)]\n",
    "                self.range_flag = False\n",
    "\n",
    "            if i % self.eval_step == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                print(f'eval Loss for epoch {i}: {sum(self.loss(y_j, y_pred[j]) for j, y_j in enumerate(y))/y.shape[0]}')\n",
    "                \n",
    "                self.hist[\"frame\"].append(go.Frame(data=[go.Scatter(x=X, y=y, name=\"train\", mode='markers'),\n",
    "                                                    go.Scatter(x=X, y=y_pred, name=\"predict\", mode='markers')]))\n",
    "                \n",
    "def plot(plot_points: Iterable[Tuple[np.ndarray, np.ndarray, str]]) -> go.Figure:\n",
    "    \"\"\"Ploting all the (X, y, plot_name) on the same figure.\"\"\"\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": False}]])\n",
    "    for X, y, name in plot_points:\n",
    "        fig.add_trace(go.Scatter(x=X, y=y, name=name, mode='markers'))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c362c349-426a-474e-a4ef-05ac2e1b0f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "traindata",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "y": [
          -40.22844191504942,
          121.19952850885748,
          144.96172544023025,
          -53.60851723261297,
          -80.78260692663481,
          71.86365061491081,
          71.46606200269252,
          -33.32518392345632,
          59.76170064071336,
          -17.71062849254889,
          -65.76873196539196,
          62.63940131898096,
          147.20992236711493,
          -11.802772177150052,
          43.96258946303781,
          -62.873106114089694,
          34.48496286551095,
          -50.94208322766583,
          -84.97715046471542,
          36.848159665703086,
          111.3695965022725,
          47.6487061159283,
          -0.09318746097468825,
          155.07843035271813,
          -37.31162587011175,
          -43.56909367306709,
          -42.212063180048965,
          72.05311045043112,
          57.94525965229457,
          47.802883472549794,
          -108.45603006941228,
          -37.358951406265206,
          -73.88924992755689,
          119.22921289759348,
          -105.96355076448256,
          107.13981775423224,
          73.87107420299392,
          138.87088623981313,
          -113.68897764193548,
          -72.55125003067819,
          145.34507920261996,
          -16.70255290513367,
          -24.168599096551084,
          -147.078116997524,
          102.94747917578634,
          13.68473739451218,
          12.847576475351495,
          -132.79641557565756,
          1.8860982974116547,
          27.1429981169738,
          5.157902013059672,
          -92.6084191604145,
          71.3230421502013,
          47.8679216790956,
          54.23228398138326,
          129.4458088465893,
          42.04992952987218,
          56.219463133367306,
          35.13054353387116,
          -43.80398048653765,
          1.995322656804564,
          -23.082101202665275,
          -68.79265216251946,
          65.45173847935665,
          -55.65121556268075,
          66.31663203773941,
          -50.15216840709069,
          20.33784411960476,
          117.8120950703883,
          -59.942744441061535,
          84.01445024244846,
          0.8589794622610386,
          -49.218442824810495,
          106.25148078477824,
          134.89849745645716,
          -115.875779070194,
          42.33389724045077,
          -70.4754911430795,
          -52.75181513404491,
          160.16900121190633,
          28.718375042976497,
          45.18520883792053,
          16.942337786719616,
          48.55083011712384,
          13.581619287641354,
          64.81342685110238,
          51.25694372869946,
          -92.85526496367903,
          -51.688566640204314,
          13.98689931493217,
          10.110839768323723,
          -112.6863029943036,
          -106.14505006269908,
          -9.678435441999309,
          -15.39672756986158,
          -77.41805720935075,
          15.978143008552935,
          -87.89705332470005,
          -22.137208364626545,
          -60.48862631037337
         ]
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          -5.891514806378132,
          104.89151480637813
         ],
         "type": "linear"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "domain": [
          0,
          1
         ],
         "range": [
          -172.4334617041275,
          185.52434591850982
         ],
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"7a34443c-53b4-4494-a9ec-e02ffbac315e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7a34443c-53b4-4494-a9ec-e02ffbac315e\")) {                    Plotly.newPlot(                        \"7a34443c-53b4-4494-a9ec-e02ffbac315e\",                        [{\"mode\":\"markers\",\"name\":\"traindata\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"y\":[-40.22844191504942,121.19952850885748,144.96172544023025,-53.60851723261297,-80.78260692663481,71.86365061491081,71.46606200269252,-33.32518392345632,59.76170064071336,-17.71062849254889,-65.76873196539196,62.63940131898096,147.20992236711493,-11.802772177150052,43.96258946303781,-62.873106114089694,34.48496286551095,-50.94208322766583,-84.97715046471542,36.848159665703086,111.3695965022725,47.6487061159283,-0.09318746097468825,155.07843035271813,-37.31162587011175,-43.56909367306709,-42.212063180048965,72.05311045043112,57.94525965229457,47.802883472549794,-108.45603006941228,-37.358951406265206,-73.88924992755689,119.22921289759348,-105.96355076448256,107.13981775423224,73.87107420299392,138.87088623981313,-113.68897764193548,-72.55125003067819,145.34507920261996,-16.70255290513367,-24.168599096551084,-147.078116997524,102.94747917578634,13.68473739451218,12.847576475351495,-132.79641557565756,1.8860982974116547,27.1429981169738,5.157902013059672,-92.6084191604145,71.3230421502013,47.8679216790956,54.23228398138326,129.4458088465893,42.04992952987218,56.219463133367306,35.13054353387116,-43.80398048653765,1.995322656804564,-23.082101202665275,-68.79265216251946,65.45173847935665,-55.65121556268075,66.31663203773941,-50.15216840709069,20.33784411960476,117.8120950703883,-59.942744441061535,84.01445024244846,0.8589794622610386,-49.218442824810495,106.25148078477824,134.89849745645716,-115.875779070194,42.33389724045077,-70.4754911430795,-52.75181513404491,160.16900121190633,28.718375042976497,45.18520883792053,16.942337786719616,48.55083011712384,13.581619287641354,64.81342685110238,51.25694372869946,-92.85526496367903,-51.688566640204314,13.98689931493217,10.110839768323723,-112.6863029943036,-106.14505006269908,-9.678435441999309,-15.39672756986158,-77.41805720935075,15.978143008552935,-87.89705332470005,-22.137208364626545,-60.48862631037337],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7a34443c-53b4-4494-a9ec-e02ffbac315e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = load_csv(\"\")\n",
    "plot(plot_points= [(np.arange(0, len(dataloader[\"X_train\"])), dataloader[\"Y_train\"].to_numpy().reshape(100), \"traindata\")] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f8639a0-d7a9-446d-867c-3b9f2b7247f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 8576.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss for epoch 0: 0.4383190819694026\n",
      "eval Loss for epoch 0: 0.2948773623457646\n",
      "Training Loss for epoch 1: 0.3190580718882809\n",
      "Training Loss for epoch 2: 0.29354849656090676\n",
      "Training Loss for epoch 3: 0.2861610870217627\n",
      "Training Loss for epoch 4: 0.28347401701999375\n",
      "Training Loss for epoch 5: 0.2823072176731065\n",
      "Training Loss for epoch 6: 0.2817177332577348\n",
      "Training Loss for epoch 7: 0.2813785384642929\n",
      "Training Loss for epoch 8: 0.2811613054206056\n",
      "Training Loss for epoch 9: 0.28100997247267934\n",
      "Training Loss for epoch 10: 0.2808975800993643\n",
      "eval Loss for epoch 10: 0.25012667225714436\n",
      "Training Loss for epoch 11: 0.2808100223167651\n",
      "Training Loss for epoch 12: 0.2807393655777836\n",
      "Training Loss for epoch 13: 0.280680864210074\n",
      "Training Loss for epoch 14: 0.28063152184172857\n",
      "Training Loss for epoch 15: 0.28058935082583475\n",
      "Training Loss for epoch 16: 0.28055296997801826\n",
      "Training Loss for epoch 17: 0.28052137666932975\n",
      "Training Loss for epoch 18: 0.2804938133328603\n",
      "Training Loss for epoch 19: 0.2804696870688741\n",
      "Training Loss for epoch 20: 0.2804485199750519\n",
      "eval Loss for epoch 20: 0.25017378641113064\n",
      "Training Loss for epoch 21: 0.28042991765066694\n",
      "Training Loss for epoch 22: 0.28041354864949763\n",
      "Training Loss for epoch 23: 0.28039913064286665\n",
      "Training Loss for epoch 24: 0.2803864207700559\n",
      "Training Loss for epoch 25: 0.28037520865633986\n",
      "Training Loss for epoch 26: 0.2803653111725555\n",
      "Training Loss for epoch 27: 0.28035656836491873\n",
      "Training Loss for epoch 28: 0.28034884019752526\n",
      "Training Loss for epoch 29: 0.28034200387972646\n",
      "Training Loss for epoch 30: 0.28033595162999064\n",
      "eval Loss for epoch 30: 0.2501824653778937\n",
      "Training Loss for epoch 31: 0.2803305887769529\n",
      "Training Loss for epoch 32: 0.280325832129077\n",
      "Training Loss for epoch 33: 0.2803216085638999\n",
      "Training Loss for epoch 34: 0.28031785380049823\n",
      "Training Loss for epoch 35: 0.2803145113272655\n",
      "Training Loss for epoch 36: 0.28031153146287446\n",
      "Training Loss for epoch 37: 0.2803088705324134\n",
      "Training Loss for epoch 38: 0.2803064901437104\n",
      "Training Loss for epoch 39: 0.2803043565511674\n",
      "Training Loss for epoch 40: 0.28030244009624605\n",
      "eval Loss for epoch 40: 0.2501853247672171\n",
      "Training Loss for epoch 41: 0.28030071471522133\n",
      "Training Loss for epoch 42: 0.2802991575060421\n",
      "Training Loss for epoch 43: 0.28029774834717025\n",
      "Training Loss for epoch 44: 0.28029646956215215\n",
      "Training Loss for epoch 45: 0.28029530562444194\n",
      "Training Loss for epoch 46: 0.2802942428976567\n",
      "Training Loss for epoch 47: 0.280293269407023\n",
      "Training Loss for epoch 48: 0.28029237463828477\n",
      "Training Loss for epoch 49: 0.2802915493607865\n",
      "Training Loss for epoch 50: 0.2802907854718376\n",
      "eval Loss for epoch 50: 0.2501863745759501\n",
      "Training Loss for epoch 51: 0.28029007585981036\n",
      "Training Loss for epoch 52: 0.2802894142837277\n",
      "Training Loss for epoch 53: 0.28028879526736367\n",
      "Training Loss for epoch 54: 0.2802882140061165\n",
      "Training Loss for epoch 55: 0.28028766628512264\n",
      "Training Loss for epoch 56: 0.2802871484072612\n",
      "Training Loss for epoch 57: 0.28028665712986206\n",
      "Training Loss for epoch 58: 0.2802861896090698\n",
      "Training Loss for epoch 59: 0.28028574335094636\n",
      "Training Loss for epoch 60: 0.2802853161684978\n",
      "eval Loss for epoch 60: 0.2501868153362967\n",
      "Training Loss for epoch 61: 0.28028490614391555\n",
      "Training Loss for epoch 62: 0.28028451159540174\n",
      "Training Loss for epoch 63: 0.2802841310480291\n",
      "Training Loss for epoch 64: 0.280283763208144\n",
      "Training Loss for epoch 65: 0.2802834069408927\n",
      "Training Loss for epoch 66: 0.28028306125048635\n",
      "Training Loss for epoch 67: 0.2802827252628788\n",
      "Training Loss for epoch 68: 0.28028239821056367\n",
      "Training Loss for epoch 69: 0.2802820794192353\n",
      "Training Loss for epoch 70: 0.2802817682960863\n",
      "eval Loss for epoch 70: 0.25018703313426943\n",
      "Training Loss for epoch 71: 0.2802814643195459\n",
      "Training Loss for epoch 72: 0.28028116703028216\n",
      "Training Loss for epoch 73: 0.28028087602331536\n",
      "Training Loss for epoch 74: 0.28028059094110946\n",
      "Training Loss for epoch 75: 0.28028031146751836\n",
      "Training Loss for epoch 76: 0.2802800373224875\n",
      "Training Loss for epoch 77: 0.2802797682574165\n",
      "Training Loss for epoch 78: 0.280279504051101\n",
      "Training Loss for epoch 79: 0.2802792445061858\n",
      "Training Loss for epoch 80: 0.28027898944606383\n",
      "eval Loss for epoch 80: 0.25018716012310926\n",
      "Training Loss for epoch 81: 0.28027873871216863\n",
      "Training Loss for epoch 82: 0.28027849216160966\n",
      "Training Loss for epoch 83: 0.28027824966511033\n",
      "Training Loss for epoch 84: 0.2802780111052097\n",
      "Training Loss for epoch 85: 0.28027777637469675\n",
      "Training Loss for epoch 86: 0.2802775453752467\n",
      "Training Loss for epoch 87: 0.280277318016236\n",
      "Training Loss for epoch 88: 0.280277094213712\n",
      "Training Loss for epoch 89: 0.2802768738894991\n",
      "Training Loss for epoch 90: 0.28027665697042414\n",
      "eval Loss for epoch 90: 0.25018724518996055\n",
      "Training Loss for epoch 91: 0.2802764433876453\n",
      "Training Loss for epoch 92: 0.28027623307607186\n",
      "Training Loss for epoch 93: 0.2802760259738634\n",
      "Training Loss for epoch 94: 0.2802758220219983\n",
      "Training Loss for epoch 95: 0.28027562116390187\n",
      "Training Loss for epoch 96: 0.28027542334512684\n",
      "Training Loss for epoch 97: 0.2802752285130812\n",
      "Training Loss for epoch 98: 0.2802750366167928\n",
      "Training Loss for epoch 99: 0.2802748476067112\n",
      "Training Loss for epoch 100: 0.2802746614345372\n",
      "eval Loss for epoch 100: 0.25018730804265155\n",
      "Training Loss for epoch 101: 0.28027447805307937\n",
      "Training Loss for epoch 102: 0.28027429741613263\n",
      "Training Loss for epoch 103: 0.28027411947837627\n",
      "Training Loss for epoch 104: 0.28027394419528784\n",
      "Training Loss for epoch 105: 0.28027377152307237\n",
      "Training Loss for epoch 106: 0.280273601418603\n",
      "Training Loss for epoch 107: 0.28027343383937253\n",
      "Training Loss for epoch 108: 0.2802732687434534\n",
      "Training Loss for epoch 109: 0.280273106089466\n",
      "Training Loss for epoch 110: 0.2802729458365531\n",
      "eval Loss for epoch 110: 0.25018735737677567\n",
      "Training Loss for epoch 111: 0.28027278794435906\n",
      "Training Loss for epoch 112: 0.2802726323730145\n",
      "Training Loss for epoch 113: 0.2802724790831248\n",
      "Training Loss for epoch 114: 0.28027232803576024\n",
      "Training Loss for epoch 115: 0.2802721791924511\n",
      "Training Loss for epoch 116: 0.28027203251518307\n",
      "Training Loss for epoch 117: 0.2802718879663942\n",
      "Training Loss for epoch 118: 0.28027174550897505\n",
      "Training Loss for epoch 119: 0.28027160510626814\n",
      "Training Loss for epoch 120: 0.2802714667220684\n",
      "eval Loss for epoch 120: 0.2501873974174015\n",
      "Training Loss for epoch 121: 0.2802713303206251\n",
      "Training Loss for epoch 122: 0.2802711958666431\n",
      "Training Loss for epoch 123: 0.280271063325285\n",
      "Training Loss for epoch 124: 0.28027093266217307\n",
      "Training Loss for epoch 125: 0.2802708038433913\n",
      "Training Loss for epoch 126: 0.2802706768354876\n",
      "Training Loss for epoch 127: 0.2802705516054749\n",
      "Training Loss for epoch 128: 0.2802704281208334\n",
      "Training Loss for epoch 129: 0.28027030634951144\n",
      "Training Loss for epoch 130: 0.2802701862599266\n",
      "eval Loss for epoch 130: 0.25018743044896696\n",
      "Training Loss for epoch 131: 0.28027006782096653\n",
      "Training Loss for epoch 132: 0.28026995100198887\n",
      "Training Loss for epoch 133: 0.280269835772822\n",
      "Training Loss for epoch 134: 0.28026972210376355\n",
      "Training Loss for epoch 135: 0.28026960996558103\n",
      "Training Loss for epoch 136: 0.2802694993295097\n",
      "Training Loss for epoch 137: 0.2802693901672517\n",
      "Training Loss for epoch 138: 0.2802692824509745\n",
      "Training Loss for epoch 139: 0.2802691761533084\n",
      "Training Loss for epoch 140: 0.2802690712473441\n",
      "eval Loss for epoch 140: 0.25018745785860136\n",
      "Training Loss for epoch 141: 0.28026896770663107\n",
      "Training Loss for epoch 142: 0.28026886550517316\n",
      "Training Loss for epoch 143: 0.28026876461742667\n",
      "Training Loss for epoch 144: 0.2802686650182963\n",
      "Training Loss for epoch 145: 0.2802685666831317\n",
      "Training Loss for epoch 146: 0.2802684695877232\n",
      "Training Loss for epoch 147: 0.2802683737082983\n",
      "Training Loss for epoch 148: 0.2802682790215169\n",
      "Training Loss for epoch 149: 0.2802681855044671\n",
      "Training Loss for epoch 150: 0.2802680931346604\n",
      "eval Loss for epoch 150: 0.25018748058966755\n",
      "Training Loss for epoch 151: 0.2802680018900274\n",
      "Training Loss for epoch 152: 0.280267911748912\n",
      "Training Loss for epoch 153: 0.2802678226900676\n",
      "Training Loss for epoch 154: 0.2802677346926506\n",
      "Training Loss for epoch 155: 0.2802676477362166\n",
      "Training Loss for epoch 156: 0.2802675618007139\n",
      "Training Loss for epoch 157: 0.2802674768664789\n",
      "Training Loss for epoch 158: 0.28026739291423025\n",
      "Training Loss for epoch 159: 0.2802673099250637\n",
      "Training Loss for epoch 160: 0.2802672278804461\n",
      "eval Loss for epoch 160: 0.250187499347502\n",
      "Training Loss for epoch 161: 0.2802671467622106\n",
      "Training Loss for epoch 162: 0.2802670665525502\n",
      "Training Loss for epoch 163: 0.28026698723401267\n",
      "Training Loss for epoch 164: 0.280266908789495\n",
      "Training Loss for epoch 165: 0.2802668312022375\n",
      "Training Loss for epoch 166: 0.2802667544558183\n",
      "Training Loss for epoch 167: 0.2802666785341478\n",
      "Training Loss for epoch 168: 0.28026660342146303\n",
      "Training Loss for epoch 169: 0.2802665291023222\n",
      "Training Loss for epoch 170: 0.2802664555615992\n",
      "eval Loss for epoch 170: 0.25018751469642597\n",
      "Training Loss for epoch 171: 0.2802663827844777\n",
      "Training Loss for epoch 172: 0.28026631075644626\n",
      "Training Loss for epoch 173: 0.28026623946329243\n",
      "Training Loss for epoch 174: 0.2802661688910978\n",
      "Training Loss for epoch 175: 0.280266099026232\n",
      "Training Loss for epoch 176: 0.28026602985534843\n",
      "Training Loss for epoch 177: 0.28026596136537785\n",
      "Training Loss for epoch 178: 0.280265893543524\n",
      "Training Loss for epoch 179: 0.2802658263772584\n",
      "Training Loss for epoch 180: 0.2802657598543147\n",
      "eval Loss for epoch 180: 0.25018752710755243\n",
      "Training Loss for epoch 181: 0.2802656939626845\n",
      "Training Loss for epoch 182: 0.28026562869061183\n",
      "Training Loss for epoch 183: 0.28026556402658803\n",
      "Training Loss for epoch 184: 0.28026549995934796\n",
      "Training Loss for epoch 185: 0.2802654364778642\n",
      "Training Loss for epoch 186: 0.2802653735713428\n",
      "Training Loss for epoch 187: 0.2802653112292184\n",
      "Training Loss for epoch 188: 0.2802652494411503\n",
      "Training Loss for epoch 189: 0.28026518819701707\n",
      "Training Loss for epoch 190: 0.2802651274869128\n",
      "eval Loss for epoch 190: 0.2501875369838077\n",
      "Training Loss for epoch 191: 0.2802650673011426\n",
      "Training Loss for epoch 192: 0.28026500763021805\n",
      "Training Loss for epoch 193: 0.28026494846485334\n",
      "Training Loss for epoch 194: 0.28026488979596076\n",
      "Training Loss for epoch 195: 0.2802648316146471\n",
      "Training Loss for epoch 196: 0.280264773912209\n",
      "Training Loss for epoch 197: 0.2802647166801294\n",
      "Training Loss for epoch 198: 0.2802646599100741\n",
      "Training Loss for epoch 199: 0.28026460359388666\n",
      "Training Loss for epoch 200: 0.2802645477235863\n",
      "eval Loss for epoch 200: 0.25018754467418475\n",
      "Training Loss for epoch 201: 0.2802644922913625\n",
      "Training Loss for epoch 202: 0.28026443728957307\n",
      "Training Loss for epoch 203: 0.2802643827107394\n",
      "Training Loss for epoch 204: 0.28026432854754346\n",
      "Training Loss for epoch 205: 0.2802642747928244\n",
      "Training Loss for epoch 206: 0.280264221439575\n",
      "Training Loss for epoch 207: 0.2802641684809386\n",
      "Training Loss for epoch 208: 0.2802641159102056\n",
      "Training Loss for epoch 209: 0.2802640637208107\n",
      "Training Loss for epoch 210: 0.2802640119063295\n",
      "eval Loss for epoch 210: 0.25018755048276914\n",
      "Training Loss for epoch 211: 0.2802639604604754\n",
      "Training Loss for epoch 212: 0.2802639093770969\n",
      "Training Loss for epoch 213: 0.28026385865017456\n",
      "Training Loss for epoch 214: 0.2802638082738178\n",
      "Training Loss for epoch 215: 0.28026375824226274\n",
      "Training Loss for epoch 216: 0.2802637085498687\n",
      "Training Loss for epoch 217: 0.28026365919111645\n",
      "Training Loss for epoch 218: 0.2802636101606044\n",
      "Training Loss for epoch 219: 0.28026356145304704\n",
      "Training Loss for epoch 220: 0.2802635130632717\n",
      "eval Loss for epoch 220: 0.2501875546751212\n",
      "Training Loss for epoch 221: 0.28026346498621657\n",
      "Training Loss for epoch 222: 0.2802634172169279\n",
      "Training Loss for epoch 223: 0.28026336975055777\n",
      "Training Loss for epoch 224: 0.2802633225823619\n",
      "Training Loss for epoch 225: 0.28026327570769693\n",
      "Training Loss for epoch 226: 0.2802632291220186\n",
      "Training Loss for epoch 227: 0.28026318282087953\n",
      "Training Loss for epoch 228: 0.2802631367999267\n",
      "Training Loss for epoch 229: 0.2802630910548999\n",
      "Training Loss for epoch 230: 0.2802630455816292\n",
      "eval Loss for epoch 230: 0.2501875574832283\n",
      "Training Loss for epoch 231: 0.28026300037603286\n",
      "Training Loss for epoch 232: 0.2802629554341164\n",
      "Training Loss for epoch 233: 0.2802629107519692\n",
      "Training Loss for epoch 234: 0.2802628663257633\n",
      "Training Loss for epoch 235: 0.28026282215175213\n",
      "Training Loss for epoch 236: 0.28026277822626744\n",
      "Training Loss for epoch 237: 0.28026273454571904\n",
      "Training Loss for epoch 238: 0.2802626911065914\n",
      "Training Loss for epoch 239: 0.2802626479054434\n",
      "Training Loss for epoch 240: 0.2802626049389059\n",
      "eval Loss for epoch 240: 0.25018755910960333\n",
      "Training Loss for epoch 241: 0.2802625622036805\n",
      "Training Loss for epoch 242: 0.2802625196965375\n",
      "Training Loss for epoch 243: 0.28026247741431465\n",
      "Training Loss for epoch 244: 0.2802624353539158\n",
      "Training Loss for epoch 245: 0.28026239351230925\n",
      "Training Loss for epoch 246: 0.2802623518865263\n",
      "Training Loss for epoch 247: 0.2802623104736594\n",
      "Training Loss for epoch 248: 0.2802622692708619\n",
      "Training Loss for epoch 249: 0.28026222827534525\n",
      "Training Loss for epoch 250: 0.2802621874843789\n",
      "eval Loss for epoch 250: 0.25018755973080864\n",
      "Training Loss for epoch 251: 0.2802621468952884\n",
      "Training Loss for epoch 252: 0.28026210650545436\n",
      "Training Loss for epoch 253: 0.2802620663123107\n",
      "Training Loss for epoch 254: 0.2802620263133443\n",
      "Training Loss for epoch 255: 0.28026198650609313\n",
      "Training Loss for epoch 256: 0.2802619468881452\n",
      "Training Loss for epoch 257: 0.28026190745713797\n",
      "Training Loss for epoch 258: 0.2802618682107563\n",
      "Training Loss for epoch 259: 0.2802618291467324\n",
      "Training Loss for epoch 260: 0.28026179026284415\n",
      "eval Loss for epoch 260: 0.25018755950054744\n",
      "Training Loss for epoch 261: 0.2802617515569137\n",
      "Training Loss for epoch 262: 0.28026171302680764\n",
      "Training Loss for epoch 263: 0.28026167467043483\n",
      "Training Loss for epoch 264: 0.28026163648574637\n",
      "Training Loss for epoch 265: 0.2802615984707338\n",
      "Training Loss for epoch 266: 0.2802615606234287\n",
      "Training Loss for epoch 267: 0.280261522941902\n",
      "Training Loss for epoch 268: 0.28026148542426244\n",
      "Training Loss for epoch 269: 0.28026144806865616\n",
      "Training Loss for epoch 270: 0.2802614108732658\n",
      "eval Loss for epoch 270: 0.2501875585524012\n",
      "Training Loss for epoch 271: 0.28026137383630967\n",
      "Training Loss for epoch 272: 0.28026133695604083\n",
      "Training Loss for epoch 273: 0.28026130023074636\n",
      "Training Loss for epoch 274: 0.280261263658747\n",
      "Training Loss for epoch 275: 0.2802612272383955\n",
      "Training Loss for epoch 276: 0.2802611909680768\n",
      "Training Loss for epoch 277: 0.28026115484620684\n",
      "Training Loss for epoch 278: 0.28026111887123184\n",
      "Training Loss for epoch 279: 0.28026108304162783\n",
      "Training Loss for epoch 280: 0.28026104735589996\n",
      "eval Loss for epoch 280: 0.25018755700226286\n",
      "Training Loss for epoch 281: 0.2802610118125819\n",
      "Training Loss for epoch 282: 0.28026097641023456\n",
      "Training Loss for epoch 283: 0.2802609411474467\n",
      "Training Loss for epoch 284: 0.28026090602283293\n",
      "Training Loss for epoch 285: 0.2802608710350346\n",
      "Training Loss for epoch 286: 0.2802608361827175\n",
      "Training Loss for epoch 287: 0.28026080146457305\n",
      "Training Loss for epoch 288: 0.2802607668793163\n",
      "Training Loss for epoch 289: 0.2802607324256865\n",
      "Training Loss for epoch 290: 0.28026069810244575\n",
      "eval Loss for epoch 290: 0.25018755495049805\n",
      "Training Loss for epoch 291: 0.28026066390837906\n",
      "Training Loss for epoch 292: 0.28026062984229305\n",
      "Training Loss for epoch 293: 0.28026059590301705\n",
      "Training Loss for epoch 294: 0.2802605620894005\n",
      "Training Loss for epoch 295: 0.28026052840031407\n",
      "Training Loss for epoch 296: 0.2802604948346487\n",
      "Training Loss for epoch 297: 0.2802604613913151\n",
      "Training Loss for epoch 298: 0.28026042806924334\n",
      "Training Loss for epoch 299: 0.28026039486738236\n",
      "Training Loss for epoch 300: 0.2802603617846997\n",
      "eval Loss for epoch 300: 0.25018755248386504\n",
      "Training Loss for epoch 301: 0.2802603288201812\n",
      "Training Loss for epoch 302: 0.28026029597283003\n",
      "Training Loss for epoch 303: 0.280260263241667\n",
      "Training Loss for epoch 304: 0.28026023062572974\n",
      "Training Loss for epoch 305: 0.28026019812407255\n",
      "Training Loss for epoch 306: 0.280260165735766\n",
      "Training Loss for epoch 307: 0.28026013345989625\n",
      "Training Loss for epoch 308: 0.28026010129556533\n",
      "Training Loss for epoch 309: 0.2802600692418902\n",
      "Training Loss for epoch 310: 0.28026003729800264\n",
      "eval Loss for epoch 310: 0.25018754967721657\n",
      "Training Loss for epoch 311: 0.28026000546304947\n",
      "Training Loss for epoch 312: 0.2802599737361908\n",
      "Training Loss for epoch 313: 0.2802599421166016\n",
      "Training Loss for epoch 314: 0.28025991060346983\n",
      "Training Loss for epoch 315: 0.280259879195997\n",
      "Training Loss for epoch 316: 0.2802598478933977\n",
      "Training Loss for epoch 317: 0.2802598166948989\n",
      "Training Loss for epoch 318: 0.28025978559974046\n",
      "Training Loss for epoch 319: 0.2802597546071743\n",
      "Training Loss for epoch 320: 0.2802597237164642\n",
      "eval Loss for epoch 320: 0.2501875465950045\n",
      "Training Loss for epoch 321: 0.28025969292688574\n",
      "Training Loss for epoch 322: 0.2802596622377258\n",
      "Training Loss for epoch 323: 0.28025963164828266\n",
      "Training Loss for epoch 324: 0.28025960115786536\n",
      "Training Loss for epoch 325: 0.2802595707657938\n",
      "Training Loss for epoch 326: 0.280259540471398\n",
      "Training Loss for epoch 327: 0.280259510274019\n",
      "Training Loss for epoch 328: 0.28025948017300734\n",
      "Training Loss for epoch 329: 0.2802594501677234\n",
      "Training Loss for epoch 330: 0.2802594202575373\n",
      "eval Loss for epoch 330: 0.2501875432926094\n",
      "Training Loss for epoch 331: 0.2802593904418288\n",
      "Training Loss for epoch 332: 0.2802593607199865\n",
      "Training Loss for epoch 333: 0.2802593310914085\n",
      "Training Loss for epoch 334: 0.28025930155550133\n",
      "Training Loss for epoch 335: 0.28025927211168045\n",
      "Training Loss for epoch 336: 0.28025924275936975\n",
      "Training Loss for epoch 337: 0.2802592134980015\n",
      "Training Loss for epoch 338: 0.28025918432701596\n",
      "Training Loss for epoch 339: 0.28025915524586154\n",
      "Training Loss for epoch 340: 0.2802591262539943\n",
      "eval Loss for epoch 340: 0.2501875398175114\n",
      "Training Loss for epoch 341: 0.28025909735087806\n",
      "Training Loss for epoch 342: 0.28025906853598426\n",
      "Training Loss for epoch 343: 0.2802590398087915\n",
      "Training Loss for epoch 344: 0.2802590111687854\n",
      "Training Loss for epoch 345: 0.28025898261545923\n",
      "Training Loss for epoch 346: 0.2802589541483125\n",
      "Training Loss for epoch 347: 0.28025892576685174\n",
      "Training Loss for epoch 348: 0.2802588974705904\n",
      "Training Loss for epoch 349: 0.28025886925904814\n",
      "Training Loss for epoch 350: 0.2802588411317508\n",
      "eval Loss for epoch 350: 0.2501875362103203\n",
      "Training Loss for epoch 351: 0.28025881308823103\n",
      "Training Loss for epoch 352: 0.28025878512802715\n",
      "Training Loss for epoch 353: 0.2802587572506834\n",
      "Training Loss for epoch 354: 0.28025872945575037\n",
      "Training Loss for epoch 355: 0.28025870174278383\n",
      "Training Loss for epoch 356: 0.28025867411134575\n",
      "Training Loss for epoch 357: 0.2802586465610033\n",
      "Training Loss for epoch 358: 0.2802586190913291\n",
      "Training Loss for epoch 359: 0.2802585917019014\n",
      "Training Loss for epoch 360: 0.28025856439230323\n",
      "eval Loss for epoch 360: 0.25018753250568065\n",
      "Training Loss for epoch 361: 0.280258537162123\n",
      "Training Loss for epoch 362: 0.28025851001095414\n",
      "Training Loss for epoch 363: 0.2802584829383949\n",
      "Training Loss for epoch 364: 0.2802584559440484\n",
      "Training Loss for epoch 365: 0.2802584290275225\n",
      "Training Loss for epoch 366: 0.2802584021884299\n",
      "Training Loss for epoch 367: 0.2802583754263875\n",
      "Training Loss for epoch 368: 0.28025834874101685\n",
      "Training Loss for epoch 369: 0.280258322131944\n",
      "Training Loss for epoch 370: 0.28025829559879906\n",
      "eval Loss for epoch 370: 0.2501875287330643\n",
      "Training Loss for epoch 371: 0.2802582691412167\n",
      "Training Loss for epoch 372: 0.2802582427588354\n",
      "Training Loss for epoch 373: 0.280258216451298\n",
      "Training Loss for epoch 374: 0.280258190218251\n",
      "Training Loss for epoch 375: 0.2802581640593453\n",
      "Training Loss for epoch 376: 0.28025813797423504\n",
      "Training Loss for epoch 377: 0.2802581119625787\n",
      "Training Loss for epoch 378: 0.28025808602403807\n",
      "Training Loss for epoch 379: 0.28025806015827887\n",
      "Training Loss for epoch 380: 0.2802580343649702\n",
      "eval Loss for epoch 380: 0.25018752491746465\n",
      "Training Loss for epoch 381: 0.28025800864378475\n",
      "Training Loss for epoch 382: 0.2802579829943989\n",
      "Training Loss for epoch 383: 0.28025795741649184\n",
      "Training Loss for epoch 384: 0.28025793190974657\n",
      "Training Loss for epoch 385: 0.2802579064738491\n",
      "Training Loss for epoch 386: 0.280257881108489\n",
      "Training Loss for epoch 387: 0.28025785581335855\n",
      "Training Loss for epoch 388: 0.28025783058815357\n",
      "Training Loss for epoch 389: 0.2802578054325723\n",
      "Training Loss for epoch 390: 0.2802577803463167\n",
      "eval Loss for epoch 390: 0.2501875210800034\n",
      "Training Loss for epoch 391: 0.28025775532909125\n",
      "Training Loss for epoch 392: 0.2802577303806034\n",
      "Training Loss for epoch 393: 0.2802577055005634\n",
      "Training Loss for epoch 394: 0.2802576806886844\n",
      "Training Loss for epoch 395: 0.2802576559446821\n",
      "Training Loss for epoch 396: 0.28025763126827524\n",
      "Training Loss for epoch 397: 0.28025760665918475\n",
      "Training Loss for epoch 398: 0.2802575821171346\n",
      "Training Loss for epoch 399: 0.2802575576418512\n",
      "Training Loss for epoch 400: 0.2802575332330631\n",
      "eval Loss for epoch 400: 0.2501875172384598\n",
      "Training Loss for epoch 401: 0.28025750889050216\n",
      "Training Loss for epoch 402: 0.28025748461390165\n",
      "Training Loss for epoch 403: 0.28025746040299804\n",
      "Training Loss for epoch 404: 0.2802574362575298\n",
      "Training Loss for epoch 405: 0.28025741217723793\n",
      "Training Loss for epoch 406: 0.2802573881618654\n",
      "Training Loss for epoch 407: 0.28025736421115766\n",
      "Training Loss for epoch 408: 0.2802573403248624\n",
      "Training Loss for epoch 409: 0.28025731650272934\n",
      "Training Loss for epoch 410: 0.2802572927445105\n",
      "eval Loss for epoch 410: 0.25018751340773215\n",
      "Training Loss for epoch 411: 0.28025726904995973\n",
      "Training Loss for epoch 412: 0.2802572454188333\n",
      "Training Loss for epoch 413: 0.2802572218508893\n",
      "Training Loss for epoch 414: 0.2802571983458878\n",
      "Training Loss for epoch 415: 0.28025717490359114\n",
      "Training Loss for epoch 416: 0.28025715152376335\n",
      "Training Loss for epoch 417: 0.2802571282061702\n",
      "Training Loss for epoch 418: 0.2802571049505798\n",
      "Training Loss for epoch 419: 0.2802570817567619\n",
      "Training Loss for epoch 420: 0.2802570586244879\n",
      "eval Loss for epoch 420: 0.2501875096002396\n",
      "Training Loss for epoch 421: 0.28025703555353154\n",
      "Training Loss for epoch 422: 0.2802570125436675\n",
      "Training Loss for epoch 423: 0.28025698959467316\n",
      "Training Loss for epoch 424: 0.2802569667063268\n",
      "Training Loss for epoch 425: 0.28025694387840894\n",
      "Training Loss for epoch 426: 0.28025692111070155\n",
      "Training Loss for epoch 427: 0.2802568984029883\n",
      "Training Loss for epoch 428: 0.2802568757550545\n",
      "Training Loss for epoch 429: 0.28025685316668697\n",
      "Training Loss for epoch 430: 0.28025683063767415\n",
      "eval Loss for epoch 430: 0.2501875058262705\n",
      "Training Loss for epoch 431: 0.280256808167806\n",
      "Training Loss for epoch 432: 0.2802567857568744\n",
      "Training Loss for epoch 433: 0.2802567634046722\n",
      "Training Loss for epoch 434: 0.2802567411109937\n",
      "Training Loss for epoch 435: 0.2802567188756354\n",
      "Training Loss for epoch 436: 0.2802566966983946\n",
      "Training Loss for epoch 437: 0.28025667457907\n",
      "Training Loss for epoch 438: 0.2802566525174622\n",
      "Training Loss for epoch 439: 0.2802566305133727\n",
      "Training Loss for epoch 440: 0.2802566085666047\n",
      "eval Loss for epoch 440: 0.2501875020942859\n",
      "Training Loss for epoch 441: 0.2802565866769625\n",
      "Training Loss for epoch 442: 0.2802565648442521\n",
      "Training Loss for epoch 443: 0.2802565430682803\n",
      "Training Loss for epoch 444: 0.2802565213488557\n",
      "Training Loss for epoch 445: 0.28025649968578786\n",
      "Training Loss for epoch 446: 0.2802564780788877\n",
      "Training Loss for epoch 447: 0.28025645652796743\n",
      "Training Loss for epoch 448: 0.28025643503284026\n",
      "Training Loss for epoch 449: 0.2802564135933212\n",
      "Training Loss for epoch 450: 0.28025639220922577\n",
      "eval Loss for epoch 450: 0.2501874984111823\n",
      "Training Loss for epoch 451: 0.2802563708803709\n",
      "Training Loss for epoch 452: 0.28025634960657503\n",
      "Training Loss for epoch 453: 0.28025632838765735\n",
      "Training Loss for epoch 454: 0.2802563072234383\n",
      "Training Loss for epoch 455: 0.2802562861137395\n",
      "Training Loss for epoch 456: 0.2802562650583835\n",
      "Training Loss for epoch 457: 0.28025624405719435\n",
      "Training Loss for epoch 458: 0.28025622310999676\n",
      "Training Loss for epoch 459: 0.28025620221661673\n",
      "Training Loss for epoch 460: 0.28025618137688135\n",
      "eval Loss for epoch 460: 0.2501874947825185\n",
      "Training Loss for epoch 461: 0.28025616059061836\n",
      "Training Loss for epoch 462: 0.2802561398576572\n",
      "Training Loss for epoch 463: 0.2802561191778277\n",
      "Training Loss for epoch 464: 0.2802560985509611\n",
      "Training Loss for epoch 465: 0.28025607797688956\n",
      "Training Loss for epoch 466: 0.2802560574554458\n",
      "Training Loss for epoch 467: 0.2802560369864644\n",
      "Training Loss for epoch 468: 0.28025601656977994\n",
      "Training Loss for epoch 469: 0.2802559962052287\n",
      "Training Loss for epoch 470: 0.2802559758926471\n",
      "eval Loss for epoch 470: 0.2501874912127127\n",
      "Training Loss for epoch 471: 0.28025595563187333\n",
      "Training Loss for epoch 472: 0.2802559354227463\n",
      "Training Loss for epoch 473: 0.2802559152651051\n",
      "Training Loss for epoch 474: 0.2802558951587907\n",
      "Training Loss for epoch 475: 0.28025587510364425\n",
      "Training Loss for epoch 476: 0.28025585509950823\n",
      "Training Loss for epoch 477: 0.2802558351462257\n",
      "Training Loss for epoch 478: 0.2802558152436406\n",
      "Training Loss for epoch 479: 0.28025579539159784\n",
      "Training Loss for epoch 480: 0.28025577558994325\n",
      "eval Loss for epoch 480: 0.2501874877052125\n",
      "Training Loss for epoch 481: 0.28025575583852325\n",
      "Training Loss for epoch 482: 0.2802557361371851\n",
      "Training Loss for epoch 483: 0.2802557164857769\n",
      "Training Loss for epoch 484: 0.28025569688414775\n",
      "Training Loss for epoch 485: 0.28025567733214735\n",
      "Training Loss for epoch 486: 0.280255657829626\n",
      "Training Loss for epoch 487: 0.2802556383764353\n",
      "Training Loss for epoch 488: 0.2802556189724272\n",
      "Training Loss for epoch 489: 0.2802555996174544\n",
      "Training Loss for epoch 490: 0.28025558031137054\n",
      "eval Loss for epoch 490: 0.25018748426264065\n",
      "Training Loss for epoch 491: 0.28025556105402993\n",
      "Training Loss for epoch 492: 0.28025554184528767\n",
      "Training Loss for epoch 493: 0.2802555226849994\n",
      "Training Loss for epoch 494: 0.28025550357302165\n",
      "Training Loss for epoch 495: 0.28025548450921167\n",
      "Training Loss for epoch 496: 0.28025546549342734\n",
      "Training Loss for epoch 497: 0.28025544652552725\n",
      "Training Loss for epoch 498: 0.2802554276053707\n",
      "Training Loss for epoch 499: 0.2802554087328176\n",
      "Training Loss for epoch 500: 0.28025538990772886\n",
      "eval Loss for epoch 500: 0.2501874808869221\n",
      "Training Loss for epoch 501: 0.28025537112996546\n",
      "Training Loss for epoch 502: 0.2802553523993896\n",
      "Training Loss for epoch 503: 0.2802553337158638\n",
      "Training Loss for epoch 504: 0.28025531507925155\n",
      "Training Loss for epoch 505: 0.2802552964894165\n",
      "Training Loss for epoch 506: 0.2802552779462235\n",
      "Training Loss for epoch 507: 0.28025525944953744\n",
      "Training Loss for epoch 508: 0.2802552409992246\n",
      "Training Loss for epoch 509: 0.2802552225951511\n",
      "Training Loss for epoch 510: 0.28025520423718425\n",
      "eval Loss for epoch 510: 0.2501874775793922\n",
      "Training Loss for epoch 511: 0.2802551859251915\n",
      "Training Loss for epoch 512: 0.28025516765904124\n",
      "Training Loss for epoch 513: 0.2802551494386024\n",
      "Training Loss for epoch 514: 0.2802551312637443\n",
      "Training Loss for epoch 515: 0.2802551131343371\n",
      "Training Loss for epoch 516: 0.2802550950502515\n",
      "Training Loss for epoch 517: 0.28025507701135854\n",
      "Training Loss for epoch 518: 0.28025505901753023\n",
      "Training Loss for epoch 519: 0.28025504106863874\n",
      "Training Loss for epoch 520: 0.28025502316455697\n",
      "eval Loss for epoch 520: 0.25018747434089\n",
      "Training Loss for epoch 521: 0.2802550053051586\n",
      "Training Loss for epoch 522: 0.2802549874903175\n",
      "Training Loss for epoch 523: 0.2802549697199082\n",
      "Training Loss for epoch 524: 0.28025495199380585\n",
      "Training Loss for epoch 525: 0.2802549343118861\n",
      "Training Loss for epoch 526: 0.2802549166740251\n",
      "Training Loss for epoch 527: 0.28025489908009954\n",
      "Training Loss for epoch 528: 0.2802548815299868\n",
      "Training Loss for epoch 529: 0.28025486402356425\n",
      "Training Loss for epoch 530: 0.2802548465607104\n",
      "eval Loss for epoch 530: 0.25018747117183826\n",
      "Training Loss for epoch 531: 0.280254829141304\n",
      "Training Loss for epoch 532: 0.2802548117652243\n",
      "Training Loss for epoch 533: 0.28025479443235113\n",
      "Training Loss for epoch 534: 0.2802547771425645\n",
      "Training Loss for epoch 535: 0.2802547598957454\n",
      "Training Loss for epoch 536: 0.28025474269177497\n",
      "Training Loss for epoch 537: 0.2802547255305351\n",
      "Training Loss for epoch 538: 0.2802547084119077\n",
      "Training Loss for epoch 539: 0.2802546913357757\n",
      "Training Loss for epoch 540: 0.280254674302022\n",
      "eval Loss for epoch 540: 0.2501874680723125\n",
      "Training Loss for epoch 541: 0.2802546573105306\n",
      "Training Loss for epoch 542: 0.28025464036118525\n",
      "Training Loss for epoch 543: 0.28025462345387075\n",
      "Training Loss for epoch 544: 0.28025460658847184\n",
      "Training Loss for epoch 545: 0.2802545897648741\n",
      "Training Loss for epoch 546: 0.2802545729829636\n",
      "Training Loss for epoch 547: 0.2802545562426264\n",
      "Training Loss for epoch 548: 0.28025453954374946\n",
      "Training Loss for epoch 549: 0.28025452288622\n",
      "Training Loss for epoch 550: 0.28025450626992565\n",
      "eval Loss for epoch 550: 0.2501874650420989\n",
      "Training Loss for epoch 551: 0.28025448969475475\n",
      "Training Loss for epoch 552: 0.2802544731605955\n",
      "Training Loss for epoch 553: 0.28025445666733706\n",
      "Training Loss for epoch 554: 0.2802544402148688\n",
      "Training Loss for epoch 555: 0.2802544238030805\n",
      "Training Loss for epoch 556: 0.2802544074318623\n",
      "Training Loss for epoch 557: 0.28025439110110506\n",
      "Training Loss for epoch 558: 0.2802543748106997\n",
      "Training Loss for epoch 559: 0.28025435856053765\n",
      "Training Loss for epoch 560: 0.28025434235051067\n",
      "eval Loss for epoch 560: 0.2501874620807448\n",
      "Training Loss for epoch 561: 0.28025432618051127\n",
      "Training Loss for epoch 562: 0.2802543100504319\n",
      "Training Loss for epoch 563: 0.28025429396016577\n",
      "Training Loss for epoch 564: 0.2802542779096063\n",
      "Training Loss for epoch 565: 0.28025426189864727\n",
      "Training Loss for epoch 566: 0.2802542459271829\n",
      "Training Loss for epoch 567: 0.2802542299951079\n",
      "Training Loss for epoch 568: 0.2802542141023172\n",
      "Training Loss for epoch 569: 0.2802541982487063\n",
      "Training Loss for epoch 570: 0.28025418243417066\n",
      "eval Loss for epoch 570: 0.25018745918760205\n",
      "Training Loss for epoch 571: 0.2802541666586068\n",
      "Training Loss for epoch 572: 0.2802541509219109\n",
      "Training Loss for epoch 573: 0.28025413522397996\n",
      "Training Loss for epoch 574: 0.2802541195647112\n",
      "Training Loss for epoch 575: 0.28025410394400224\n",
      "Training Loss for epoch 576: 0.2802540883617511\n",
      "Training Loss for epoch 577: 0.2802540728178559\n",
      "Training Loss for epoch 578: 0.28025405731221553\n",
      "Training Loss for epoch 579: 0.28025404184472896\n",
      "Training Loss for epoch 580: 0.28025402641529545\n",
      "eval Loss for epoch 580: 0.25018745636186235\n",
      "Training Loss for epoch 581: 0.2802540110238149\n",
      "Training Loss for epoch 582: 0.28025399567018733\n",
      "Training Loss for epoch 583: 0.28025398035431315\n",
      "Training Loss for epoch 584: 0.2802539650760931\n",
      "Training Loss for epoch 585: 0.2802539498354285\n",
      "Training Loss for epoch 586: 0.2802539346322205\n",
      "Training Loss for epoch 587: 0.280253919466371\n",
      "Training Loss for epoch 588: 0.28025390433778224\n",
      "Training Loss for epoch 589: 0.28025388924635647\n",
      "Training Loss for epoch 590: 0.2802538741919967\n",
      "eval Loss for epoch 590: 0.250187453602589\n",
      "Training Loss for epoch 591: 0.28025385917460577\n",
      "Training Loss for epoch 592: 0.28025384419408744\n",
      "Training Loss for epoch 593: 0.2802538292503452\n",
      "Training Loss for epoch 594: 0.2802538143432832\n",
      "Training Loss for epoch 595: 0.280253799472806\n",
      "Training Loss for epoch 596: 0.280253784638818\n",
      "Training Loss for epoch 597: 0.2802537698412245\n",
      "Training Loss for epoch 598: 0.2802537550799309\n",
      "Training Loss for epoch 599: 0.2802537403548426\n",
      "Training Loss for epoch 600: 0.2802537256658658\n",
      "eval Loss for epoch 600: 0.2501874509087429\n",
      "Training Loss for epoch 601: 0.2802537110129067\n",
      "Training Loss for epoch 602: 0.28025369639587183\n",
      "Training Loss for epoch 603: 0.28025368181466814\n",
      "Training Loss for epoch 604: 0.2802536672692028\n",
      "Training Loss for epoch 605: 0.2802536527593834\n",
      "Training Loss for epoch 606: 0.28025363828511773\n",
      "Training Loss for epoch 607: 0.2802536238463137\n",
      "Training Loss for epoch 608: 0.28025360944287986\n",
      "Training Loss for epoch 609: 0.2802535950747248\n",
      "Training Loss for epoch 610: 0.28025358074175777\n",
      "eval Loss for epoch 610: 0.2501874482792044\n",
      "Training Loss for epoch 611: 0.28025356644388766\n",
      "Training Loss for epoch 612: 0.2802535521810242\n",
      "Training Loss for epoch 613: 0.2802535379530773\n",
      "Training Loss for epoch 614: 0.28025352375995705\n",
      "Training Loss for epoch 615: 0.2802535096015738\n",
      "Training Loss for epoch 616: 0.28025349547783823\n",
      "Training Loss for epoch 617: 0.2802534813886616\n",
      "Training Loss for epoch 618: 0.28025346733395473\n",
      "Training Loss for epoch 619: 0.2802534533136295\n",
      "Training Loss for epoch 620: 0.2802534393275976\n",
      "eval Loss for epoch 620: 0.2501874457127929\n",
      "Training Loss for epoch 621: 0.2802534253757712\n",
      "Training Loss for epoch 622: 0.2802534114580626\n",
      "Training Loss for epoch 623: 0.28025339757438433\n",
      "Training Loss for epoch 624: 0.2802533837246495\n",
      "Training Loss for epoch 625: 0.2802533699087711\n",
      "Training Loss for epoch 626: 0.2802533561266628\n",
      "Training Loss for epoch 627: 0.28025334237823807\n",
      "Training Loss for epoch 628: 0.280253328663411\n",
      "Training Loss for epoch 629: 0.280253314982096\n",
      "Training Loss for epoch 630: 0.28025330133420734\n",
      "eval Loss for epoch 630: 0.25018744320828157\n",
      "Training Loss for epoch 631: 0.28025328771965985\n",
      "Training Loss for epoch 632: 0.2802532741383685\n",
      "Training Loss for epoch 633: 0.2802532605902486\n",
      "Training Loss for epoch 634: 0.28025324707521576\n",
      "Training Loss for epoch 635: 0.2802532335931858\n",
      "Training Loss for epoch 636: 0.28025322014407467\n",
      "Training Loss for epoch 637: 0.2802532067277987\n",
      "Training Loss for epoch 638: 0.28025319334427456\n",
      "Training Loss for epoch 639: 0.2802531799934188\n",
      "Training Loss for epoch 640: 0.2802531666751487\n",
      "eval Loss for epoch 640: 0.2501874407644117\n",
      "Training Loss for epoch 641: 0.2802531533893816\n",
      "Training Loss for epoch 642: 0.2802531401360348\n",
      "Training Loss for epoch 643: 0.28025312691502624\n",
      "Training Loss for epoch 644: 0.28025311372627404\n",
      "Training Loss for epoch 645: 0.28025310056969643\n",
      "Training Loss for epoch 646: 0.2802530874452118\n",
      "Training Loss for epoch 647: 0.28025307435273916\n",
      "Training Loss for epoch 648: 0.2802530612921973\n",
      "Training Loss for epoch 649: 0.2802530482635055\n",
      "Training Loss for epoch 650: 0.28025303526658346\n",
      "eval Loss for epoch 650: 0.2501874383799028\n",
      "Training Loss for epoch 651: 0.28025302230135063\n",
      "Training Loss for epoch 652: 0.28025300936772707\n",
      "Training Loss for epoch 653: 0.28025299646563306\n",
      "Training Loss for epoch 654: 0.2802529835949888\n",
      "Training Loss for epoch 655: 0.2802529707557151\n",
      "Training Loss for epoch 656: 0.2802529579477328\n",
      "Training Loss for epoch 657: 0.28025294517096316\n",
      "Training Loss for epoch 658: 0.28025293242532734\n",
      "Training Loss for epoch 659: 0.28025291971074684\n",
      "Training Loss for epoch 660: 0.28025290702714384\n",
      "eval Loss for epoch 660: 0.2501874360534627\n",
      "Training Loss for epoch 661: 0.2802528943744399\n",
      "Training Loss for epoch 662: 0.2802528817525576\n",
      "Training Loss for epoch 663: 0.28025286916141934\n",
      "Training Loss for epoch 664: 0.2802528566009476\n",
      "Training Loss for epoch 665: 0.28025284407106565\n",
      "Training Loss for epoch 666: 0.28025283157169634\n",
      "Training Loss for epoch 667: 0.2802528191027633\n",
      "Training Loss for epoch 668: 0.2802528066641897\n",
      "Training Loss for epoch 669: 0.2802527942558997\n",
      "Training Loss for epoch 670: 0.28025278187781716\n",
      "eval Loss for epoch 670: 0.2501874337837947\n",
      "Training Loss for epoch 671: 0.28025276952986644\n",
      "Training Loss for epoch 672: 0.2802527572119718\n",
      "Training Loss for epoch 673: 0.2802527449240579\n",
      "Training Loss for epoch 674: 0.2802527326660497\n",
      "Training Loss for epoch 675: 0.2802527204378723\n",
      "Training Loss for epoch 676: 0.280252708239451\n",
      "Training Loss for epoch 677: 0.28025269607071107\n",
      "Training Loss for epoch 678: 0.28025268393157854\n",
      "Training Loss for epoch 679: 0.28025267182197916\n",
      "Training Loss for epoch 680: 0.2802526597418392\n",
      "eval Loss for epoch 680: 0.2501874315696042\n",
      "Training Loss for epoch 681: 0.2802526476910848\n",
      "Training Loss for epoch 682: 0.2802526356696427\n",
      "Training Loss for epoch 683: 0.2802526236774394\n",
      "Training Loss for epoch 684: 0.280252611714402\n",
      "Training Loss for epoch 685: 0.2802525997804578\n",
      "Training Loss for epoch 686: 0.280252587875534\n",
      "Training Loss for epoch 687: 0.28025257599955794\n",
      "Training Loss for epoch 688: 0.28025256415245786\n",
      "Training Loss for epoch 689: 0.2802525523341613\n",
      "Training Loss for epoch 690: 0.2802525405445967\n",
      "eval Loss for epoch 690: 0.2501874294096037\n",
      "Training Loss for epoch 691: 0.2802525287836922\n",
      "Training Loss for epoch 692: 0.28025251705137655\n",
      "Training Loss for epoch 693: 0.2802525053475783\n",
      "Training Loss for epoch 694: 0.2802524936722266\n",
      "Training Loss for epoch 695: 0.28025248202525055\n",
      "Training Loss for epoch 696: 0.2802524704065793\n",
      "Training Loss for epoch 697: 0.28025245881614275\n",
      "Training Loss for epoch 698: 0.28025244725387033\n",
      "Training Loss for epoch 699: 0.280252435719692\n",
      "Training Loss for epoch 700: 0.2802524242135379\n",
      "eval Loss for epoch 700: 0.25018742730251725\n",
      "Training Loss for epoch 701: 0.28025241273533835\n",
      "Training Loss for epoch 702: 0.280252401285024\n",
      "Training Loss for epoch 703: 0.28025238986252515\n",
      "Training Loss for epoch 704: 0.28025237846777296\n",
      "Training Loss for epoch 705: 0.2802523671006984\n",
      "Training Loss for epoch 706: 0.28025235576123275\n",
      "Training Loss for epoch 707: 0.2802523444493074\n",
      "Training Loss for epoch 708: 0.2802523331648541\n",
      "Training Loss for epoch 709: 0.28025232190780447\n",
      "Training Loss for epoch 710: 0.2802523106780906\n",
      "eval Loss for epoch 710: 0.2501874252470838\n",
      "Training Loss for epoch 711: 0.2802522994756446\n",
      "Training Loss for epoch 712: 0.2802522883003989\n",
      "Training Loss for epoch 713: 0.28025227715228596\n",
      "Training Loss for epoch 714: 0.28025226603123854\n",
      "Training Loss for epoch 715: 0.28025225493718964\n",
      "Training Loss for epoch 716: 0.28025224387007214\n",
      "Training Loss for epoch 717: 0.2802522328298195\n",
      "Training Loss for epoch 718: 0.2802522218163649\n",
      "Training Loss for epoch 719: 0.28025221082964225\n",
      "Training Loss for epoch 720: 0.2802521998695852\n",
      "eval Loss for epoch 720: 0.2501874232420599\n",
      "Training Loss for epoch 721: 0.28025218893612763\n",
      "Training Loss for epoch 722: 0.280252178029204\n",
      "Training Loss for epoch 723: 0.28025216714874823\n",
      "Training Loss for epoch 724: 0.28025215629469513\n",
      "Training Loss for epoch 725: 0.2802521454669793\n",
      "Training Loss for epoch 726: 0.2802521346655355\n",
      "Training Loss for epoch 727: 0.28025212389029885\n",
      "Training Loss for epoch 728: 0.2802521131412045\n",
      "Training Loss for epoch 729: 0.28025210241818777\n",
      "Training Loss for epoch 730: 0.28025209172118437\n",
      "eval Loss for epoch 730: 0.25018742128622207\n",
      "Training Loss for epoch 731: 0.2802520810501298\n",
      "Training Loss for epoch 732: 0.2802520704049601\n",
      "Training Loss for epoch 733: 0.2802520597856112\n",
      "Training Loss for epoch 734: 0.2802520491920194\n",
      "Training Loss for epoch 735: 0.28025203862412107\n",
      "Training Loss for epoch 736: 0.28025202808185284\n",
      "Training Loss for epoch 737: 0.2802520175651512\n",
      "Training Loss for epoch 738: 0.28025200707395337\n",
      "Training Loss for epoch 739: 0.2802519966081961\n",
      "Training Loss for epoch 740: 0.2802519861678169\n",
      "eval Loss for epoch 740: 0.2501874193783683\n",
      "Training Loss for epoch 741: 0.2802519757527529\n",
      "Training Loss for epoch 742: 0.2802519653629418\n",
      "Training Loss for epoch 743: 0.28025195499832134\n",
      "Training Loss for epoch 744: 0.28025194465882924\n",
      "Training Loss for epoch 745: 0.2802519343444037\n",
      "Training Loss for epoch 746: 0.28025192405498295\n",
      "Training Loss for epoch 747: 0.2802519137905051\n",
      "Training Loss for epoch 748: 0.280251903550909\n",
      "Training Loss for epoch 749: 0.28025189333613315\n",
      "Training Loss for epoch 750: 0.28025188314611654\n",
      "eval Loss for epoch 750: 0.25018741751731954\n",
      "Training Loss for epoch 751: 0.28025187298079807\n",
      "Training Loss for epoch 752: 0.28025186284011694\n",
      "Training Loss for epoch 753: 0.2802518527240123\n",
      "Training Loss for epoch 754: 0.2802518426324239\n",
      "Training Loss for epoch 755: 0.28025183256529135\n",
      "Training Loss for epoch 756: 0.28025182252255443\n",
      "Training Loss for epoch 757: 0.28025181250415293\n",
      "Training Loss for epoch 758: 0.28025180251002724\n",
      "Training Loss for epoch 759: 0.2802517925401174\n",
      "Training Loss for epoch 760: 0.280251782594364\n",
      "eval Loss for epoch 760: 0.25018741570192027\n",
      "Training Loss for epoch 761: 0.2802517726727074\n",
      "Training Loss for epoch 762: 0.2802517627750887\n",
      "Training Loss for epoch 763: 0.28025175290144844\n",
      "Training Loss for epoch 764: 0.28025174305172773\n",
      "Training Loss for epoch 765: 0.2802517332258678\n",
      "Training Loss for epoch 766: 0.28025172342381016\n",
      "Training Loss for epoch 767: 0.28025171364549617\n",
      "Training Loss for epoch 768: 0.2802517038908673\n",
      "Training Loss for epoch 769: 0.2802516941598656\n",
      "Training Loss for epoch 770: 0.2802516844524329\n",
      "eval Loss for epoch 770: 0.2501874139310396\n",
      "Training Loss for epoch 771: 0.2802516747685114\n",
      "Training Loss for epoch 772: 0.2802516651080432\n",
      "Training Loss for epoch 773: 0.280251655470971\n",
      "Training Loss for epoch 774: 0.28025164585723694\n",
      "Training Loss for epoch 775: 0.28025163626678395\n",
      "Training Loss for epoch 776: 0.2802516266995549\n",
      "Training Loss for epoch 777: 0.2802516171554926\n",
      "Training Loss for epoch 778: 0.2802516076345403\n",
      "Training Loss for epoch 779: 0.28025159813664124\n",
      "Training Loss for epoch 780: 0.28025158866173905\n",
      "eval Loss for epoch 780: 0.25018741220357155\n",
      "Training Loss for epoch 781: 0.28025157920977695\n",
      "Training Loss for epoch 782: 0.280251569780699\n",
      "Training Loss for epoch 783: 0.2802515603744488\n",
      "Training Loss for epoch 784: 0.28025155099097043\n",
      "Training Loss for epoch 785: 0.280251541630208\n",
      "Training Loss for epoch 786: 0.2802515322921061\n",
      "Training Loss for epoch 787: 0.2802515229766087\n",
      "Training Loss for epoch 788: 0.28025151368366075\n",
      "Training Loss for epoch 789: 0.28025150441320673\n",
      "Training Loss for epoch 790: 0.2802514951651917\n",
      "eval Loss for epoch 790: 0.25018741051843496\n",
      "Training Loss for epoch 791: 0.28025148593956045\n",
      "Training Loss for epoch 792: 0.2802514767362582\n",
      "Training Loss for epoch 793: 0.28025146755523034\n",
      "Training Loss for epoch 794: 0.2802514583964221\n",
      "Training Loss for epoch 795: 0.28025144925977924\n",
      "Training Loss for epoch 796: 0.2802514401452474\n",
      "Training Loss for epoch 797: 0.28025143105277217\n",
      "Training Loss for epoch 798: 0.2802514219823\n",
      "Training Loss for epoch 799: 0.2802514129337766\n",
      "Training Loss for epoch 800: 0.2802514039071483\n",
      "eval Loss for epoch 800: 0.2501874088745741\n",
      "Training Loss for epoch 801: 0.2802513949023616\n",
      "Training Loss for epoch 802: 0.28025138591936305\n",
      "Training Loss for epoch 803: 0.280251376958099\n",
      "Training Loss for epoch 804: 0.28025136801851663\n",
      "Training Loss for epoch 805: 0.28025135910056265\n",
      "Training Loss for epoch 806: 0.2802513502041842\n",
      "Training Loss for epoch 807: 0.2802513413293284\n",
      "Training Loss for epoch 808: 0.2802513324759426\n",
      "Training Loss for epoch 809: 0.2802513236439744\n",
      "Training Loss for epoch 810: 0.28025131483337123\n",
      "eval Loss for epoch 810: 0.25018740727095806\n",
      "Training Loss for epoch 811: 0.28025130604408094\n",
      "Training Loss for epoch 812: 0.28025129727605136\n",
      "Training Loss for epoch 813: 0.2802512885292304\n",
      "Training Loss for epoch 814: 0.28025127980356634\n",
      "Training Loss for epoch 815: 0.28025127109900727\n",
      "Training Loss for epoch 816: 0.2802512624155017\n",
      "Training Loss for epoch 817: 0.28025125375299803\n",
      "Training Loss for epoch 818: 0.28025124511144517\n",
      "Training Loss for epoch 819: 0.28025123649079164\n",
      "Training Loss for epoch 820: 0.28025122789098633\n",
      "eval Loss for epoch 820: 0.25018740570658105\n",
      "Training Loss for epoch 821: 0.28025121931197844\n",
      "Training Loss for epoch 822: 0.2802512107537172\n",
      "Training Loss for epoch 823: 0.28025120221615163\n",
      "Training Loss for epoch 824: 0.28025119369923135\n",
      "Training Loss for epoch 825: 0.2802511852029059\n",
      "Training Loss for epoch 826: 0.28025117672712485\n",
      "Training Loss for epoch 827: 0.28025116827183805\n",
      "Training Loss for epoch 828: 0.2802511598369956\n",
      "Training Loss for epoch 829: 0.2802511514225473\n",
      "Training Loss for epoch 830: 0.28025114302844356\n",
      "eval Loss for epoch 830: 0.25018740418046187\n",
      "Training Loss for epoch 831: 0.28025113465463447\n",
      "Training Loss for epoch 832: 0.2802511263010706\n",
      "Training Loss for epoch 833: 0.28025111796770247\n",
      "Training Loss for epoch 834: 0.28025110965448075\n",
      "Training Loss for epoch 835: 0.28025110136135634\n",
      "Training Loss for epoch 836: 0.28025109308828\n",
      "Training Loss for epoch 837: 0.28025108483520295\n",
      "Training Loss for epoch 838: 0.2802510766020762\n",
      "Training Loss for epoch 839: 0.28025106838885117\n",
      "Training Loss for epoch 840: 0.2802510601954793\n",
      "eval Loss for epoch 840: 0.2501874026916436\n",
      "Training Loss for epoch 841: 0.28025105202191203\n",
      "Training Loss for epoch 842: 0.28025104386810107\n",
      "Training Loss for epoch 843: 0.28025103573399823\n",
      "Training Loss for epoch 844: 0.2802510276195554\n",
      "Training Loss for epoch 845: 0.28025101952472464\n",
      "Training Loss for epoch 846: 0.2802510114494581\n",
      "Training Loss for epoch 847: 0.2802510033937079\n",
      "Training Loss for epoch 848: 0.2802509953574266\n",
      "Training Loss for epoch 849: 0.28025098734056664\n",
      "Training Loss for epoch 850: 0.28025097934308063\n",
      "eval Loss for epoch 850: 0.25018740123919336\n",
      "Training Loss for epoch 851: 0.2802509713649214\n",
      "Training Loss for epoch 852: 0.2802509634060416\n",
      "Training Loss for epoch 853: 0.28025095546639445\n",
      "Training Loss for epoch 854: 0.28025094754593294\n",
      "Training Loss for epoch 855: 0.28025093964461034\n",
      "Training Loss for epoch 856: 0.28025093176237986\n",
      "Training Loss for epoch 857: 0.2802509238991951\n",
      "Training Loss for epoch 858: 0.28025091605500946\n",
      "Training Loss for epoch 859: 0.28025090822977683\n",
      "Training Loss for epoch 860: 0.2802509004234507\n",
      "eval Loss for epoch 860: 0.2501873998222017\n",
      "Training Loss for epoch 861: 0.28025089263598535\n",
      "Training Loss for epoch 862: 0.2802508848673346\n",
      "Training Loss for epoch 863: 0.28025087711745256\n",
      "Training Loss for epoch 864: 0.2802508693862935\n",
      "Training Loss for epoch 865: 0.28025086167381186\n",
      "Training Loss for epoch 866: 0.2802508539799621\n",
      "Training Loss for epoch 867: 0.2802508463046989\n",
      "Training Loss for epoch 868: 0.2802508386479767\n",
      "Training Loss for epoch 869: 0.2802508310097507\n",
      "Training Loss for epoch 870: 0.2802508233899756\n",
      "eval Loss for epoch 870: 0.2501873984397822\n",
      "Training Loss for epoch 871: 0.2802508157886064\n",
      "Training Loss for epoch 872: 0.28025080820559833\n",
      "Training Loss for epoch 873: 0.2802508006409068\n",
      "Training Loss for epoch 874: 0.28025079309448697\n",
      "Training Loss for epoch 875: 0.2802507855662945\n",
      "Training Loss for epoch 876: 0.28025077805628495\n",
      "Training Loss for epoch 877: 0.280250770564414\n",
      "Training Loss for epoch 878: 0.28025076309063746\n",
      "Training Loss for epoch 879: 0.2802507556349113\n",
      "Training Loss for epoch 880: 0.28025074819719165\n",
      "eval Loss for epoch 880: 0.25018739709107135\n",
      "Training Loss for epoch 881: 0.2802507407774346\n",
      "Training Loss for epoch 882: 0.28025073337559636\n",
      "Training Loss for epoch 883: 0.2802507259916334\n",
      "Training Loss for epoch 884: 0.28025071862550194\n",
      "Training Loss for epoch 885: 0.28025071127715895\n",
      "Training Loss for epoch 886: 0.2802507039465608\n",
      "Training Loss for epoch 887: 0.2802506966336646\n",
      "Training Loss for epoch 888: 0.2802506893384271\n",
      "Training Loss for epoch 889: 0.2802506820608053\n",
      "Training Loss for epoch 890: 0.2802506748007562\n",
      "eval Loss for epoch 890: 0.25018739577522714\n",
      "Training Loss for epoch 891: 0.28025066755823735\n",
      "Training Loss for epoch 892: 0.2802506603332058\n",
      "Training Loss for epoch 893: 0.2802506531256193\n",
      "Training Loss for epoch 894: 0.2802506459354349\n",
      "Training Loss for epoch 895: 0.2802506387626108\n",
      "Training Loss for epoch 896: 0.2802506316071046\n",
      "Training Loss for epoch 897: 0.280250624468874\n",
      "Training Loss for epoch 898: 0.28025061734787704\n",
      "Training Loss for epoch 899: 0.2802506102440718\n",
      "Training Loss for epoch 900: 0.28025060315741646\n",
      "eval Loss for epoch 900: 0.2501873944914297\n",
      "Training Loss for epoch 901: 0.2802505960878694\n",
      "Training Loss for epoch 902: 0.2802505890353889\n",
      "Training Loss for epoch 903: 0.2802505819999334\n",
      "Training Loss for epoch 904: 0.2802505749814616\n",
      "Training Loss for epoch 905: 0.28025056797993214\n",
      "Training Loss for epoch 906: 0.2802505609953039\n",
      "Training Loss for epoch 907: 0.2802505540275356\n",
      "Training Loss for epoch 908: 0.28025054707658636\n",
      "Training Loss for epoch 909: 0.2802505401424154\n",
      "Training Loss for epoch 910: 0.2802505332249816\n",
      "eval Loss for epoch 910: 0.25018739323887995\n",
      "Training Loss for epoch 911: 0.28025052632424463\n",
      "Training Loss for epoch 912: 0.28025051944016355\n",
      "Training Loss for epoch 913: 0.2802505125726981\n",
      "Training Loss for epoch 914: 0.28025050572180793\n",
      "Training Loss for epoch 915: 0.28025049888745246\n",
      "Training Loss for epoch 916: 0.28025049206959174\n",
      "Training Loss for epoch 917: 0.28025048526818547\n",
      "Training Loss for epoch 918: 0.2802504784831939\n",
      "Training Loss for epoch 919: 0.28025047171457695\n",
      "Training Loss for epoch 920: 0.2802504649622949\n",
      "eval Loss for epoch 920: 0.25018739201679946\n",
      "Training Loss for epoch 921: 0.2802504582263082\n",
      "Training Loss for epoch 922: 0.2802504515065768\n",
      "Training Loss for epoch 923: 0.2802504448030616\n",
      "Training Loss for epoch 924: 0.28025043811572303\n",
      "Training Loss for epoch 925: 0.28025043144452183\n",
      "Training Loss for epoch 926: 0.28025042478941875\n",
      "Training Loss for epoch 927: 0.28025041815037477\n",
      "Training Loss for epoch 928: 0.28025041152735075\n",
      "Training Loss for epoch 929: 0.28025040492030767\n",
      "Training Loss for epoch 930: 0.28025039832920706\n",
      "eval Loss for epoch 930: 0.2501873908244298\n",
      "Training Loss for epoch 931: 0.28025039175400973\n",
      "Training Loss for epoch 932: 0.2802503851946775\n",
      "Training Loss for epoch 933: 0.28025037865117153\n",
      "Training Loss for epoch 934: 0.2802503721234534\n",
      "Training Loss for epoch 935: 0.2802503656114848\n",
      "Training Loss for epoch 936: 0.2802503591152275\n",
      "Training Loss for epoch 937: 0.2802503526346434\n",
      "Training Loss for epoch 938: 0.28025034616969424\n",
      "Training Loss for epoch 939: 0.28025033972034236\n",
      "Training Loss for epoch 940: 0.28025033328654947\n",
      "eval Loss for epoch 940: 0.2501873896610322\n",
      "Training Loss for epoch 941: 0.2802503268682781\n",
      "Training Loss for epoch 942: 0.28025032046549037\n",
      "Training Loss for epoch 943: 0.280250314078149\n",
      "Training Loss for epoch 944: 0.2802503077062161\n",
      "Training Loss for epoch 945: 0.2802503013496544\n",
      "Training Loss for epoch 946: 0.28025029500842646\n",
      "Training Loss for epoch 947: 0.28025028868249535\n",
      "Training Loss for epoch 948: 0.2802502823718236\n",
      "Training Loss for epoch 949: 0.28025027607637437\n",
      "Training Loss for epoch 950: 0.2802502697961107\n",
      "eval Loss for epoch 950: 0.25018738852588684\n",
      "Training Loss for epoch 951: 0.2802502635309956\n",
      "Training Loss for epoch 952: 0.2802502572809924\n",
      "Training Loss for epoch 953: 0.28025025104606427\n",
      "Training Loss for epoch 954: 0.28025024482617483\n",
      "Training Loss for epoch 955: 0.28025023862128734\n",
      "Training Loss for epoch 956: 0.28025023243136565\n",
      "Training Loss for epoch 957: 0.28025022625637325\n",
      "Training Loss for epoch 958: 0.280250220096274\n",
      "Training Loss for epoch 959: 0.2802502139510316\n",
      "Training Loss for epoch 960: 0.2802502078206103\n",
      "eval Loss for epoch 960: 0.25018738741829233\n",
      "Training Loss for epoch 961: 0.2802502017049738\n",
      "Training Loss for epoch 962: 0.2802501956040863\n",
      "Training Loss for epoch 963: 0.28025018951791203\n",
      "Training Loss for epoch 964: 0.2802501834464154\n",
      "Training Loss for epoch 965: 0.28025017738956076\n",
      "Training Loss for epoch 966: 0.2802501713473125\n",
      "Training Loss for epoch 967: 0.28025016531963515\n",
      "Training Loss for epoch 968: 0.28025015930649355\n",
      "Training Loss for epoch 969: 0.2802501533078523\n",
      "Training Loss for epoch 970: 0.2802501473236761\n",
      "eval Loss for epoch 970: 0.25018738633756554\n",
      "Training Loss for epoch 971: 0.2802501413539301\n",
      "Training Loss for epoch 972: 0.28025013539857907\n",
      "Training Loss for epoch 973: 0.28025012945758826\n",
      "Training Loss for epoch 974: 0.28025012353092277\n",
      "Training Loss for epoch 975: 0.2802501176185478\n",
      "Training Loss for epoch 976: 0.2802501117204288\n",
      "Training Loss for epoch 977: 0.28025010583653115\n",
      "Training Loss for epoch 978: 0.2802500999668202\n",
      "Training Loss for epoch 979: 0.2802500941112618\n",
      "Training Loss for epoch 980: 0.28025008826982145\n",
      "eval Loss for epoch 980: 0.25018738528304063\n",
      "Training Loss for epoch 981: 0.2802500824424649\n",
      "Training Loss for epoch 982: 0.2802500766291582\n",
      "Training Loss for epoch 983: 0.280250070829867\n",
      "Training Loss for epoch 984: 0.2802500650445575\n",
      "Training Loss for epoch 985: 0.2802500592731957\n",
      "Training Loss for epoch 986: 0.28025005351574783\n",
      "Training Loss for epoch 987: 0.2802500477721802\n",
      "Training Loss for epoch 988: 0.28025004204245896\n",
      "Training Loss for epoch 989: 0.28025003632655077\n",
      "Training Loss for epoch 990: 0.280250030624422\n",
      "eval Loss for epoch 990: 0.25018738425406944\n",
      "Training Loss for epoch 991: 0.2802500249360393\n",
      "Training Loss for epoch 992: 0.2802500192613694\n",
      "Training Loss for epoch 993: 0.2802500136003788\n",
      "Training Loss for epoch 994: 0.28025000795303456\n",
      "Training Loss for epoch 995: 0.28025000231930364\n",
      "Training Loss for epoch 996: 0.28024999669915285\n",
      "Training Loss for epoch 997: 0.28024999109254944\n",
      "Training Loss for epoch 998: 0.2802499854994605\n",
      "Training Loss for epoch 999: 0.28024997991985323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# activation function and its derivative\n",
    "tanh = lambda x: np.tanh(x)\n",
    "tanh_prime = lambda x: 1-np.tanh(x)**2 \n",
    "mse = lambda y, y_pred: np.mean((y-y_pred)**2)\n",
    "mse_prime = lambda y, y_pred : 2*(y_pred - y)/y.shape[0];\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# x_train = dataloader[\"X_train\"].to_numpy().reshape((dataloader[\"X_train\"].shape[0],1,dataloader[\"X_train\"].shape[1]))\n",
    "# y_train = dataloader[\"Y_train\"].to_numpy().reshape((dataloader[\"Y_train\"].shape[0],1,dataloader[\"Y_train\"].shape[1]))\n",
    "\n",
    "# network\n",
    "model = MLP(optimizer= None, hist= {}, learning_rate=0.1,\n",
    "            epochs = 1000, eval_step=10, early_stop= False,\n",
    "            range_flag= False, loss= mse, loss_prime=mse_prime,\n",
    "            layers=[\n",
    "                FCLayer(input_size=x_train.shape[2], output_size=3, \n",
    "                            activation=tanh, activation_prime=tanh_prime,\n",
    "                        ),\n",
    "                FCLayer(input_size=3, output_size=1, \n",
    "                            activation=tanh, activation_prime=tanh_prime,\n",
    "                        ),\n",
    "                   ],\n",
    "            )\n",
    "\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eb06d80-485e-4b94-99f3-23e88211f765",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[ 0.0187423 ]\n",
      " [-0.08377632]\n",
      " [ 0.07978293]\n",
      " [-0.13177034]\n",
      " [ 0.22361747]\n",
      " [ 0.09283401]\n",
      " [-0.25815805]\n",
      " [-0.26488927]\n",
      " [-0.23948734]\n",
      " [ 0.03140907]], db= [[-0.59261206]]\n",
      "dW = [[-0.  0. -0.  0. -0.  0.  0.  0. -0. -0.]\n",
      " [-0.  0. -0.  0. -0.  0.  0.  0. -0. -0.]], db= [[-0.02485893  0.27306975 -0.02439339  0.08974973 -0.01158514  0.21333425\n",
      "   0.11130554  0.16774881 -0.19340832 -0.07161404]]\n",
      "dW = [[ 0.0661716 ]\n",
      " [ 0.35038733]\n",
      " [ 0.87322062]\n",
      " [ 0.04341744]\n",
      " [-0.10091153]\n",
      " [-0.27006913]\n",
      " [-0.7172301 ]\n",
      " [-0.29739816]\n",
      " [-1.28232392]\n",
      " [-0.74550195]], db= [[-1.75321986]]\n",
      "dW = [[-0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-7.02316865e-02  7.77314086e-01 -4.47483557e-02  2.56072389e-01\n",
      "  -7.57064132e-04  6.47555539e-01  3.00716598e-01  5.57240048e-01\n",
      "  -3.37555217e-01 -1.69537304e-01]], db= [[-7.02316865e-02  7.77314086e-01 -4.47483557e-02  2.56072389e-01\n",
      "  -7.57064132e-04  6.47555539e-01  3.00716598e-01  5.57240048e-01\n",
      "  -3.37555217e-01 -1.69537304e-01]]\n",
      "dW = [[-0.24943942]\n",
      " [-0.18552899]\n",
      " [ 0.3536101 ]\n",
      " [ 0.09388678]\n",
      " [ 0.48097515]\n",
      " [ 0.49382587]\n",
      " [-0.29026862]\n",
      " [-0.43426151]\n",
      " [-0.82649344]\n",
      " [ 0.31301148]], db= [[-1.21960148]]\n",
      "dW = [[-0.03914604  0.59193369  0.05962731  0.18244981 -0.01083928  0.35822524\n",
      "   0.15448521  0.3168457  -0.35761856 -0.21941645]\n",
      " [-0.          0.          0.          0.         -0.          0.\n",
      "   0.          0.         -0.         -0.        ]], db= [[-0.03914604  0.59193369  0.05962731  0.18244981 -0.01083928  0.35822524\n",
      "   0.15448521  0.3168457  -0.35761856 -0.21941645]]\n",
      "dW = [[ 0.1136666 ]\n",
      " [-0.18660071]\n",
      " [-0.33301174]\n",
      " [-0.19639974]\n",
      " [ 0.02088601]\n",
      " [-0.12273924]\n",
      " [ 0.08594059]\n",
      " [-0.02136367]\n",
      " [ 0.47968309]\n",
      " [ 0.15644059]], db= [[0.53998564]]\n",
      "dW = [[ 0.03015983 -0.22743054 -0.02969347 -0.07491142 -0.02025859 -0.20519586\n",
      "  -0.05539273 -0.13698945  0.07115792  0.07978579]\n",
      " [ 0.03015983 -0.22743054 -0.02969347 -0.07491142 -0.02025859 -0.20519586\n",
      "  -0.05539273 -0.13698945  0.07115792  0.07978579]], db= [[ 0.03015983 -0.22743054 -0.02969347 -0.07491142 -0.02025859 -0.20519586\n",
      "  -0.05539273 -0.13698945  0.07115792  0.07978579]]\n",
      "epoch 1/1   error=0.507828\n",
      "[array([[0.33486055]]), array([[0.64470186]]), array([[0.52046123]]), array([[0.73419993]])]\n"
     ]
    }
   ],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \n",
    "        # print(f\"error.shape= {output_error.shape}\")\n",
    "        # print(f\"error= {output_error.shape}, W.T= {self.weights.T.shape}\")\n",
    "        # print(f\"x.T={self.input.T.shape}, error= {output_error.shape}\")\n",
    "       \n",
    "        \n",
    "        \n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        print(f\"dW = {weights_error}, db= {output_error}\")\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "    \n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            \n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# x_train = dataloader[\"X_train\"].to_numpy().reshape((dataloader[\"X_train\"].shape[0],1,dataloader[\"X_train\"].shape[1]))\n",
    "# y_train = dataloader[\"Y_train\"].to_numpy().reshape((dataloader[\"Y_train\"].shape[0],1,dataloader[\"Y_train\"].shape[1]))\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(x_train.shape[2], 10))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(10, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-course",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
